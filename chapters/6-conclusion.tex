This thesis described efforts in developing a \emph{software tool for authoring mid-air gestures} to support the activities of end-users. For this purpose, through guidelines derived from the literature and a user-centered design process, a paradigm based on space discretization for visualizing and declaratively manipulating mid-air gesture information was developed. This paradigm was implemented in Hotspotizer, a standalone Windows application that maps mid-air gestures to commands issued from an emulated keyboard.Hotspotizer was evaluated through a user study and class workshop.

Findings from the evaluation sessions verify that Hotspotizer observes its design rationale and supports gesture authoring for end-users. Using Hotspotizer, \emph{gestural interactions were implemented within minutes by users who did not have the skills} to use textual programming tools. Usage strategies and users' choices for gesture designs implied that users understood the \emph{domain expertise} embedded in the interface and leveraged their \emph{sense of personal space and proprioception} in interacting with the system. Hotspotizer was used to control other programs on a PC, making use of a \emph{common infrastructure}.

The space discretization paradigm may have value for authoring gestures enabled using technologies other than skeletal tracking. I encourage other researchers to adopt the paradigm for use in different contexts.

\section{Future Work}
\label{sec:future-work}

The research described in this thesis instigates a number of opportunities for future work.

\subsection{Expanding Hotspotizer}

One strand of future work may deal with expanding the expressive power of Hotspotizer by implementing new features in a user-friendly manner.

While it did not come up in the user studies, I find that the current visualization style may become convoluted as gesture collections grow in size. Exploring alternative ways of visualizing many gestures within one workspace is on our agenda for future versions of the software.

Currently, (as I discussed in Section~\ref{sec:hotspotizer}) Hotspotizer does not directly support "online" (REF) --- i.e. continuous --- gesturing, since it adopts a traditional event-based model for detecting and responding to gestures. As such, support for \emph{manipulative} and \emph{deictic} gestures, which are common across gesture-based user interfaces, is severely limited. As \textcite{Myers:2000} recommend, ideally, the "continuous nature of the input [should] be preserved." This, however, requires "tighter integration with application logic" \parencite{Hartmann:2007} through interfacing with a textual programming language or third-party applications integrating support for continuous input streams. Unfortunately,  the first option oversteps the scope of the Hotspotizer project (See Section~\ref{sec:formative-studies}). The second option can be explored for a limited set of third-party applications.

Among other features are negative hotspots that mark space that should not be engaged when gesturing (i.e. negation \parencite{Hoste:2014}), a movable frame of reference for the workspace to enable gesturing around peripheral body parts, resizable hotspot boundaries, adjustable timeout, compositions that involve multiple limbs, and recognition of hand movements.  As implied by user studies, the capability to infer hotspots from \emph{demonstration}, and \emph{speech recognition} to control the application from a distance are features that may further accelerate user workflows.

Incorporating classifier-coupled gesture recognition \parencite{Hoste:2013} could serve to alleviate recognizer errors \parencite{Myers:2000}, and, when needed, to decouple overlapping gesture definitions.
